{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next word based on last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-datasets tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = tf.data.Dataset.from_tensor_slices([\"primo secondo terzo quarto quinto sesto settimo ottavo nono decimo\"])\n",
    "\n",
    "for element in small_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_text_content = \"\"\n",
    "with open(\"./notebooks/il-piccolo-principe.txt\", \"r\") as file:\n",
    "    file_text_content = file.read()\n",
    "file_text_content = re.sub(r'[^a-zA-ZàéòùìèÈ]', ' ', file_text_content)\n",
    "medium_dataset = tf.data.Dataset.from_tensor_slices([file_text_content])\n",
    "\n",
    "for element in medium_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset = tfds.load('wikipedia', split='train').map(lambda x: x['text']).take(200)\n",
    "\n",
    "for element in large_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = large_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe use subword tokenizer https://www.tensorflow.org/text/guide/subwords_tokenizer\n",
    "\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(dataset)\n",
    "\n",
    "END_OF_SEQUENCE_TOKEN_INDEX = text_vectorizer.vocabulary_size()\n",
    "text_vectorizer.set_vocabulary(text_vectorizer.get_vocabulary() + [\"[END_OF_SEQUENCE]\"])\n",
    "vocabulary_size = text_vectorizer.vocabulary_size() + 1\n",
    "\n",
    "print(text_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_WINDOW_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_and_pad_sequence(sequence):\n",
    "    assert len(sequence.shape) == 1\n",
    "    sequence = sequence[-SEQUENCE_WINDOW_SIZE:]\n",
    "    sequence = tf.pad(sequence, [[max(0, SEQUENCE_WINDOW_SIZE - sequence.shape[0]), 0]])\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_samples():\n",
    "    for whole_text_token_indices in dataset.map(text_vectorizer):\n",
    "        for i in range(len(whole_text_token_indices)):\n",
    "            yield (\n",
    "                truncate_and_pad_sequence(whole_text_token_indices[:i]),\n",
    "                whole_text_token_indices[i],\n",
    "            ) \n",
    "        yield (\n",
    "            truncate_and_pad_sequence(whole_text_token_indices),\n",
    "            END_OF_SEQUENCE_TOKEN_INDEX,\n",
    "        )\n",
    "\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    generate_training_samples,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(SEQUENCE_WINDOW_SIZE,), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64),\n",
    "    ),\n",
    ").cache()\n",
    "\n",
    "for (input, output) in training_dataset.take(50):\n",
    "    print(([text_vectorizer.get_vocabulary()[token_index] for token_index in input], text_vectorizer.get_vocabulary()[output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(SEQUENCE_WINDOW_SIZE), dtype=tf.int64)\n",
    "\n",
    "word_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocabulary_size,\n",
    "    output_dim=31,  # smallest common word embedding dimensionality\n",
    ")\n",
    "\n",
    "def positional_encoding_layer(inputs):\n",
    "  return tf.concat(\n",
    "    [\n",
    "        inputs,\n",
    "        tf.tile(\n",
    "            (tf.range(SEQUENCE_WINDOW_SIZE, dtype=tf.float32) / SEQUENCE_WINDOW_SIZE)[\n",
    "                tf.newaxis, :, tf.newaxis\n",
    "            ],\n",
    "            multiples=[tf.shape(layer)[0], 1, 1],\n",
    "        ),\n",
    "    ],\n",
    "    axis=-1,\n",
    ")\n",
    "\n",
    "def processing_layer(layer, levels = 4):\n",
    "    for _ in range(levels):\n",
    "        layer = tf.concat([layer, tf.keras.layers.Dense(32, activation=tf.nn.relu)(layer)], axis=-1)\n",
    "    return layer\n",
    "\n",
    "def attention_layer(layer):\n",
    "    return tf.reduce_mean(processing_layer(layer), axis=-2)\n",
    "\n",
    "token_selector_layer = tf.keras.layers.Dense(vocabulary_size, activation=tf.nn.softmax)\n",
    "\n",
    "layer = input_layer\n",
    "layer = word_embedding_layer(layer)\n",
    "last = layer[..., -1, :]\n",
    "layer = positional_encoding_layer(layer)\n",
    "layer = attention_layer(layer)\n",
    "layer = tf.concat([layer, last], axis=-1)\n",
    "layer = processing_layer(layer)\n",
    "layer = token_selector_layer(layer)\n",
    "\n",
    "output_layer = layer\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    training_dataset.prefetch(tf.data.AUTOTUNE).batch(64),\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_completion(input_text, max_inferred_tokens=50):\n",
    "    input_token_indices = text_vectorizer(input_text)\n",
    "    output_token_indices = tf.constant([], dtype=tf.int64)\n",
    "    while len(output_token_indices) < max_inferred_tokens:\n",
    "        model_input = truncate_and_pad_sequence(\n",
    "            tf.concat([input_token_indices, output_token_indices], axis=0)\n",
    "        )\n",
    "        token_indices_probabilities = model.predict(model_input[tf.newaxis, :])[0]\n",
    "        next_token_index = tf.argmax(token_indices_probabilities)\n",
    "        if next_token_index == END_OF_SEQUENCE_TOKEN_INDEX:\n",
    "            break\n",
    "        output_token_indices = tf.concat(\n",
    "            [output_token_indices, [next_token_index]], axis=0\n",
    "        )\n",
    "    output_text = \" \".join([text_vectorizer.get_vocabulary()[token_index] for token_index in output_token_indices])\n",
    "    return (input_text, output_text, len(output_token_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_completion(\"she believed\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
