{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next word based on last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-datasets tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = tf.data.Dataset.from_tensor_slices([\"primo secondo terzo quarto quinto sesto settimo ottavo nono decimo\"])\n",
    "\n",
    "for element in small_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text_content = \"\"\n",
    "with open(\"./notebooks/il-piccolo-principe.txt\", \"r\") as file:\n",
    "    file_text_content = file.read()\n",
    "medium_dataset = tf.data.Dataset.from_tensor_slices(file_text_content.split(\"\\n\"))\n",
    "\n",
    "for element in medium_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dataset = tfds.load('wikipedia', split='train').map(lambda x: x['text']).take(100)\n",
    "\n",
    "for element in large_dataset.take(1):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe use subword tokenizer https://www.tensorflow.org/text/guide/subwords_tokenizer\n",
    "\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(dataset)\n",
    "\n",
    "END_OF_SEQUENCE_TOKEN_INDEX = text_vectorizer.vocabulary_size()\n",
    "text_vectorizer.set_vocabulary(text_vectorizer.get_vocabulary() + [\"[END_OF_SEQUENCE]\"])\n",
    "vocabulary_size = text_vectorizer.vocabulary_size() + 1\n",
    "\n",
    "print(text_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_WINDOW_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_and_pad_sequence(sequence):\n",
    "    assert len(sequence.shape) == 1\n",
    "    sequence = sequence[-SEQUENCE_WINDOW_SIZE:]\n",
    "    sequence = tf.pad(sequence, [[max(0, SEQUENCE_WINDOW_SIZE - sequence.shape[0]), 0]])\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_samples():\n",
    "    for whole_text_token_indices in dataset.map(text_vectorizer):\n",
    "        for i in range(len(whole_text_token_indices)):\n",
    "            yield (\n",
    "                truncate_and_pad_sequence(whole_text_token_indices[:i]),\n",
    "                whole_text_token_indices[i],\n",
    "            )\n",
    "        yield (\n",
    "            truncate_and_pad_sequence(whole_text_token_indices),\n",
    "            END_OF_SEQUENCE_TOKEN_INDEX,\n",
    "        )\n",
    "\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    generate_training_samples,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(SEQUENCE_WINDOW_SIZE,), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64),\n",
    "    ),\n",
    ").cache()\n",
    "\n",
    "for (input, output) in training_dataset.take(10):\n",
    "    print(([text_vectorizer.get_vocabulary()[token_index] for token_index in input], text_vectorizer.get_vocabulary()[output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(SEQUENCE_WINDOW_SIZE), dtype=tf.int64)\n",
    "\n",
    "word_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocabulary_size,\n",
    "    output_dim=32, # smallest common word embedding dimensionality\n",
    ")\n",
    "\n",
    "token_selector_layer = tf.keras.layers.Dense(vocabulary_size, activation=tf.nn.softmax)\n",
    "\n",
    "layer = input_layer\n",
    "layer = word_embedding_layer(layer)\n",
    "layer = layer[..., -1, :]\n",
    "layer = token_selector_layer(layer)\n",
    "\n",
    "output_layer = layer\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    training_dataset.prefetch(tf.data.AUTOTUNE).batch(256),\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_completion(input_text, max_inferred_tokens=10):\n",
    "    input_token_indices = text_vectorizer(input_text)\n",
    "    output_token_indices = tf.constant([], dtype=tf.int64)\n",
    "    while len(output_token_indices) < max_inferred_tokens:\n",
    "        model_input = truncate_and_pad_sequence(\n",
    "            tf.concat([input_token_indices, output_token_indices], axis=0)\n",
    "        )\n",
    "        token_indices_probabilities = model.predict(model_input[tf.newaxis, :])[0]\n",
    "        next_token_index = tf.argmax(token_indices_probabilities)\n",
    "        if next_token_index == END_OF_SEQUENCE_TOKEN_INDEX:\n",
    "            break\n",
    "        output_token_indices = tf.concat(\n",
    "            [output_token_indices, [next_token_index]], axis=0\n",
    "        )\n",
    "    output_text = \" \".join([text_vectorizer.get_vocabulary()[token_index] for token_index in output_token_indices])\n",
    "    return (input_text, output_text, len(output_token_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_completion(\"primo\"))\n",
    "print(text_completion(\"secondo\"))\n",
    "print(text_completion(\"terzo\"))\n",
    "print(text_completion(\"quarto\"))\n",
    "print(text_completion(\"quinto\"))\n",
    "print(text_completion(\"sesto\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
