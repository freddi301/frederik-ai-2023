{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function huber\n",
    "# optimizer adam or it's variants\n",
    "# activation hidden relu to get non-linearity\n",
    "# activation final softmax to get probabilities\n",
    "# argmax to get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open('./notebooks/il-piccolo-principe.txt', 'r') as file:\n",
    "  text = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def simplify_italian_text(text):\n",
    "  # Convert text to lowercase\n",
    "  text = text.lower()\n",
    "  # Normalize special characters\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "  # Remove punctuation\n",
    "  text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "  text = re.sub(r'\\d', ' ', text)\n",
    "  # Remove extra whitespace\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = simplify_italian_text(text)\n",
    "print(text)\n",
    "print(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def string_to_int_array(text):\n",
    "  unique_chars = sorted(set(text))\n",
    "  dictionary_size = len(unique_chars)\n",
    "  char_to_int = {char: i for i, char in enumerate(unique_chars)}\n",
    "  int_to_char = {i: char for i, char in enumerate(unique_chars)}\n",
    "  int_array = np.array([char_to_int[char] for char in text])\n",
    "  return int_array, dictionary_size, lambda x: char_to_int[x], lambda x: int_to_char[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dictionary_size, char_to_int, int_to_char = string_to_int_array(text)\n",
    "for i in range(len(text)):\n",
    "  assert char_to_int(text[i]) == data[i]\n",
    "  assert int_to_char(data[i]) == text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_array(int_array, num_classes):\n",
    "  one_hot = np.zeros((int_array.shape[0], num_classes))\n",
    "  one_hot[np.arange(int_array.shape[0]), int_array] = 1\n",
    "  return one_hot\n",
    "\n",
    "def one_hot_decode_array(one_hot_array):\n",
    "  return np.argmax(one_hot_array, axis=1)\n",
    "\n",
    "def one_hot_encode(int, num_classes):\n",
    "  one_hot = np.zeros(num_classes)\n",
    "  one_hot[int] = 1\n",
    "  return one_hot\n",
    "\n",
    "def one_hot_decode(one_hot):\n",
    "  return np.argmax(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "\n",
    "def one_hot_to_x(data_one_hot):\n",
    "  assert data_one_hot.shape[0] >= window_size\n",
    "  x = np.array([data_one_hot[i:i+window_size] for i in range(data_one_hot.shape[0] - window_size + 1)])\n",
    "  return x\n",
    "\n",
    "data_one_hot = one_hot_encode_array(data, dictionary_size)\n",
    "assert data_one_hot.shape[0] == data.shape[0]\n",
    "\n",
    "x = one_hot_to_x(data_one_hot[0:-1])\n",
    "assert x.shape[0] == data_one_hot.shape[0] - window_size\n",
    "\n",
    "y = data_one_hot[window_size:]\n",
    "assert x.shape[0] == y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  # tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(window_size, dictionary_size)),\n",
    "  # tf.keras.layers.Flatten(input_shape=(window_size, dictionary_size)),\n",
    "  tf.keras.layers.Dense(2048, activation='relu', input_shape=(window_size, dictionary_size)),\n",
    "  tf.keras.layers.Dense(1024, activation='relu',),\n",
    "  tf.keras.layers.Dense(512, activation='relu',),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(dictionary_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='huber', optimizer='adam')\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=5, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_one_hot(one_hot_sequence):\n",
    "  probabilities = model.predict(one_hot_to_x(one_hot_sequence)[-1:])[0]\n",
    "  return one_hot_encode(np.argmax(probabilities), dictionary_size)\n",
    "\n",
    "def generate_text(starting_text, num_chars):\n",
    "  one_hot_sequence = one_hot_encode_array(np.array([char_to_int(char) for char in starting_text]), dictionary_size)\n",
    "  for i in range(num_chars):\n",
    "    one_hot_sequence = np.append(one_hot_sequence, np.array([predict_next_one_hot(one_hot_sequence)]), axis=0)\n",
    "  fullText = ''.join([int_to_char(one_hot_decode(one_hot)) for one_hot in one_hot_sequence])\n",
    "  return fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = generate_text('per sommo piacere'.rjust(window_size), 1000)\n",
    "print(p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
